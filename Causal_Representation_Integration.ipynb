{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dchu26/Causal-Representation-Learning-with-Partial-Observability/blob/main/Causal_Representation_Integration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "9jhR8HkT6xcD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7k0uyL83iwr"
      },
      "outputs": [],
      "source": [
        "# Install dependencies (usually first cell)\n",
        "!pip install -q medmnist\n",
        "!pip install -U medmnist seaborn\n",
        "!pip install causal-learn\n",
        "\n",
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from sklearn.manifold import TSNE\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from medmnist import INFO, OrganMNIST3D\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Prep"
      ],
      "metadata": {
        "id": "oNhdlLy-7tp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class OrganMultiViewDataset(Dataset):\n",
        "    def __init__(self, split: str, transform=None):\n",
        "        from medmnist import OrganMNIST3D\n",
        "        self.raw = OrganMNIST3D(split=split, download=True)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.raw)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        volume, label = self.raw[idx]\n",
        "        vol = np.squeeze(volume)         # (D, H, W)\n",
        "        d, h, w = vol.shape\n",
        "        axial    = vol[d//2, :, :]       # axial slice at center\n",
        "        coronal  = vol[:, h//2, :]       # coronal slice\n",
        "        sagittal = vol[:, :, w//2]       # sagittal slice\n",
        "        # Apply transforms (e.g., ToTensor, Normalize)\n",
        "        if self.transform:\n",
        "            axial    = self.transform(axial)\n",
        "            coronal  = self.transform(coronal)\n",
        "            sagittal = self.transform(sagittal)\n",
        "        else:\n",
        "            axial    = torch.tensor(axial, dtype=torch.float32).unsqueeze(0)\n",
        "            coronal  = torch.tensor(coronal, dtype=torch.float32).unsqueeze(0)\n",
        "            sagittal = torch.tensor(sagittal, dtype=torch.float32).unsqueeze(0)\n",
        "        return axial, coronal, sagittal, torch.tensor(label, dtype=torch.long).squeeze()\n",
        "\n",
        "# Example transforms: convert to tensor and normalize\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),                 # HxW -> 1xHxW, scales to [0,1]\n",
        "    transforms.Normalize((0.5,), (0.5,))   # mean=0.5, std=0.5\n",
        "])\n",
        "\n",
        "# Instantiate datasets and dataloaders\n",
        "train_ds = OrganMultiViewDataset(split='train', transform=transform)\n",
        "val_ds   = OrganMultiViewDataset(split='val',   transform=transform)\n",
        "test_ds  = OrganMultiViewDataset(split='test',  transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "OQ2v35N047Fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(val_ds))"
      ],
      "metadata": {
        "id": "FSAoOxBMlKzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Architecture"
      ],
      "metadata": {
        "id": "2NE2gi688JN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNEncoder(nn.Module):\n",
        "    def __init__(self, out_features=128):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),          # 28x28 -> 14x14\n",
        "            nn.Conv2d(32, 64, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),          # 14x14 -> 7x7\n",
        "            nn.Conv2d(64, 128, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d(1)   # 7x7 -> 1x1\n",
        "        )\n",
        "        self.fc = nn.Linear(128, out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)                # shape (batch, 128, 1, 1)\n",
        "        x = x.view(x.size(0), -1)       # flatten to (batch, 128)\n",
        "        x = self.fc(x)                  # (batch, latent_dim)\n",
        "        return x\n",
        "\n",
        "# Multi-view network combining the three encoders and a classifier\n",
        "class MultiViewNet(nn.Module):\n",
        "    def __init__(self, latent_dim=128, num_classes=11, return_latent=False):\n",
        "        super().__init__()\n",
        "        self.encoder_axial    = CNNEncoder(out_features=latent_dim)\n",
        "        self.encoder_coronal  = CNNEncoder(out_features=latent_dim)\n",
        "        self.encoder_sagittal = CNNEncoder(out_features=latent_dim)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "        self.return_latent = return_latent  # controls whether we return the fused latent vector\n",
        "\n",
        "    def forward(self, axial, coronal, sagittal, mask, return_latent=False):\n",
        "        f_ax = self.encoder_axial(axial)\n",
        "        f_co = self.encoder_coronal(coronal)\n",
        "        f_sa = self.encoder_sagittal(sagittal)\n",
        "\n",
        "        f_ax = f_ax * mask[:, 0].unsqueeze(1)\n",
        "        f_co = f_co * mask[:, 1].unsqueeze(1)\n",
        "        f_sa = f_sa * mask[:, 2].unsqueeze(1)\n",
        "\n",
        "        sum_mask = mask.sum(dim=1).unsqueeze(1)\n",
        "        fused = (f_ax + f_co + f_sa) / (sum_mask + 1e-6)\n",
        "\n",
        "        out = self.classifier(fused)\n",
        "\n",
        "        if return_latent:\n",
        "            return out, fused\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = MultiViewNet(latent_dim=128, num_classes=11, return_latent=True).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "CrgmKqU-Iqa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalRegularizer(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super().__init__()\n",
        "        self.A = nn.Parameter(torch.randn(latent_dim, latent_dim) * 0.01)\n",
        "\n",
        "    def acyclicity_loss(self):\n",
        "        \"\"\"Implements the NOTEARS acyclicity constraint\"\"\"\n",
        "        d = self.A.shape[0]\n",
        "        expm = torch.matrix_exp(self.A * self.A)  # elementwise square then matrix exp\n",
        "        return torch.trace(expm) - d\n",
        "\n",
        "    def forward(self, Z):\n",
        "        # Optional: enforce sparsity or structure on A, e.g. L1 penalty\n",
        "        return self.acyclicity_loss()\n"
      ],
      "metadata": {
        "id": "dMjA-NWA_dBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Masking Data"
      ],
      "metadata": {
        "id": "RWjf-4TQ8Ne5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_masks(batch_size, mode):\n",
        "    \"\"\"\n",
        "    Create a mask tensor (batch_size, 3) based on mode:\n",
        "      - 'full': all ones,\n",
        "      - 'dual': exactly two ones,\n",
        "      - 'single': exactly one one,\n",
        "      - 'random': 1 to 3 ones randomly.\n",
        "    \"\"\"\n",
        "    mask = torch.zeros((batch_size, 3), dtype=torch.float32)\n",
        "    for i in range(batch_size):\n",
        "        if mode == 'full':\n",
        "            mask[i] = torch.tensor([1, 1, 1], dtype=torch.float32)\n",
        "        elif mode == 'dual':\n",
        "            keep = random.sample([0, 1, 2], 2)\n",
        "            mask[i, keep] = 1.0\n",
        "        elif mode == 'single':\n",
        "            keep = random.choice([0, 1, 2])\n",
        "            mask[i, keep] = 1.0\n",
        "        else:  # random\n",
        "            k = random.choice([1, 2, 3])\n",
        "            keep = random.sample([0, 1, 2], k)\n",
        "            mask[i, keep] = 1.0\n",
        "    return mask\n",
        "\n",
        "causal_reg = CausalRegularizer(latent_dim=128).to(device)\n",
        "lambda_causal = 0.1  # tune this weight\n",
        "\n",
        "num_epochs = 80\n",
        "for epoch in range(num_epochs):\n",
        "    # Determine mask mode per epoch\n",
        "    if epoch < 20:\n",
        "        mode = 'full'\n",
        "    elif epoch < 40:\n",
        "        mode = 'dual'\n",
        "    elif epoch < 60:\n",
        "        mode = 'single'\n",
        "    else:\n",
        "        mode = 'random'\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for axial, coronal, sagittal, labels in train_loader:\n",
        "        axial = axial.to(device).float()\n",
        "        coronal = coronal.to(device).float()\n",
        "        sagittal = sagittal.to(device).float()\n",
        "        labels = labels.to(device)\n",
        "        mask = create_masks(labels.size(0), mode).to(device)\n",
        "\n",
        "        # Forward pass with latent output\n",
        "        outputs, latents = model(axial, coronal, sagittal, mask, return_latent=True)\n",
        "\n",
        "        loss_cls = criterion(outputs, labels)\n",
        "        loss_causal = causal_reg(latents)\n",
        "        loss = loss_cls + lambda_causal * loss_causal\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(train_ds)\n",
        "\n",
        "    # Validation (full views, no causal loss)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for axial, coronal, sagittal, labels in val_loader:\n",
        "            axial = axial.to(device).float()\n",
        "            coronal = coronal.to(device).float()\n",
        "            sagittal = sagittal.to(device).float()\n",
        "            labels = labels.to(device)\n",
        "            mask = torch.ones((labels.size(0), 3), dtype=torch.float32).to(device)\n",
        "            outputs = model(axial, coronal, sagittal, mask)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    val_acc = 100.0 * correct / total\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Val Acc (full): {val_acc:.2f}%\")\n"
      ],
      "metadata": {
        "id": "dR3KeFzvIvd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "pFaVpNDW8UJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "for eval_mode in ['full', 'dual', 'single', 'random']:\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for axial, coronal, sagittal, labels in test_loader:\n",
        "            axial = axial.to(device).float()\n",
        "            coronal = coronal.to(device).float()\n",
        "            sagittal = sagittal.to(device).float()\n",
        "            labels = labels.to(device)\n",
        "            mask = create_masks(labels.size(0), eval_mode).to(device)\n",
        "            outputs = model(axial, coronal, sagittal, mask)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    acc = 100.0 * np.mean(np.array(all_preds) == np.array(all_labels))\n",
        "    print(f\"Test Accuracy ({eval_mode} views): {acc:.2f}%\")\n",
        "    # Plot confusion matrix for the 'full' case\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f'Confusion Matrix ({eval_mode.capitalize()} Views)')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "WqVACrpKI1WA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import torch.nn.functional as F  # for softmax\n",
        "\n",
        "model.eval()\n",
        "num_classes = 11\n",
        "\n",
        "\n",
        "for eval_mode in ['full', 'dual', 'single', 'random']:\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_logits = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for axial, coronal, sagittal, labels in test_loader:\n",
        "            axial = axial.to(device).float()\n",
        "            coronal = coronal.to(device).float()\n",
        "            sagittal = sagittal.to(device).float()\n",
        "            labels = labels.to(device)\n",
        "            mask = create_masks(labels.size(0), eval_mode).to(device)\n",
        "\n",
        "            outputs = model(axial, coronal, sagittal, mask)  # shape: [batch_size, num_classes]\n",
        "            probs = F.softmax(outputs, dim=1)  # convert logits to probabilities\n",
        "            preds = probs.argmax(dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_logits.extend(probs.cpu().numpy())  # store softmaxed outputs\n",
        "\n",
        "    # Accuracy\n",
        "    acc = 100.0 * np.mean(np.array(all_preds) == np.array(all_labels))\n",
        "    print(f\"\\nTest Accuracy ({eval_mode} views): {acc:.2f}%\")\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f'Confusion Matrix ({eval_mode.capitalize()} Views)')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # ROC-AUC\n",
        "    y_true = label_binarize(all_labels, classes=list(range(num_classes)))  # one-hot\n",
        "    y_score = np.array(all_logits)  # probabilities from softmax\n",
        "\n",
        "    try:\n",
        "        macro_roc_auc = roc_auc_score(y_true, y_score, average='macro', multi_class='ovr')\n",
        "        print(f\"ROC-AUC (macro) ({eval_mode} views): {macro_roc_auc:.4f}\")\n",
        "\n",
        "        # ROC curves per class\n",
        "        fpr = dict()\n",
        "        tpr = dict()\n",
        "        roc_auc = dict()\n",
        "        for i in range(num_classes):\n",
        "            fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_score[:, i])\n",
        "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "        # Plot ROC curves\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        for i in range(num_classes):\n",
        "            plt.plot(fpr[i], tpr[i], lw=2, label=f'Class {i} (AUC = {roc_auc[i]:.2f})')\n",
        "        plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title(f'ROC Curves ({eval_mode.capitalize()} Views)')\n",
        "        plt.legend(loc='lower right', fontsize='small')\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(f\"Could not compute ROC-AUC for {eval_mode}: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "A37eyL5P6oYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Causal Discovery"
      ],
      "metadata": {
        "id": "lPYBhNpVQiew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "all_latents = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for axial, coronal, sagittal, labels in test_loader:\n",
        "        axial = axial.to(device).float()\n",
        "        coronal = coronal.to(device).float()\n",
        "        sagittal = sagittal.to(device).float()\n",
        "        mask = torch.ones((labels.size(0), 3), dtype=torch.float32).to(device)\n",
        "        _, latents = model(axial, coronal, sagittal, mask, return_latent=True)\n",
        "\n",
        "        all_latents.append(latents.cpu())\n",
        "        all_labels.append(labels.cpu())\n",
        "\n",
        "latents_all = torch.cat(all_latents).numpy()\n",
        "labels_all = torch.cat(all_labels).numpy()"
      ],
      "metadata": {
        "id": "CmyW4F7gzPcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from causallearn.search.ConstraintBased.PC import pc\n",
        "from causallearn.utils.GraphUtils import GraphUtils\n",
        "from causallearn.utils.cit import fisherz\n",
        "\n",
        "# Run PC with Fisher's Z test, appropriate for continuous variables\n",
        "pc_result = pc(\n",
        "    data=latents_all,\n",
        "    alpha=0.05,  # significance level for independence tests\n",
        "    ci_test=fisherz\n",
        ")"
      ],
      "metadata": {
        "id": "d5HphrS1pPrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def draw_curved_edges(G, pos, rad=0.25, **kwargs):\n",
        "    \"\"\"Draw curved edges for bidirectional edges\"\"\"\n",
        "    seen = set()\n",
        "    ax = plt.gca()\n",
        "    for (u, v) in G.edges():\n",
        "        if (v, u) in G.edges() and (v, u) not in seen:\n",
        "            # draw curved edge one way\n",
        "            rad_pos = rad\n",
        "            rad_neg = -rad\n",
        "            # edge u->v curved one way\n",
        "            ax.annotate(\"\",\n",
        "                        xy=pos[v], xycoords='data',\n",
        "                        xytext=pos[u], textcoords='data',\n",
        "                        arrowprops=dict(arrowstyle='-|>', color=kwargs.get('edge_color', 'k'),\n",
        "                                        shrinkA=10, shrinkB=10,\n",
        "                                        patchA=None, patchB=None,\n",
        "                                        connectionstyle=f\"arc3,rad={rad_pos}\"),\n",
        "                        )\n",
        "            # edge v->u curved other way\n",
        "            ax.annotate(\"\",\n",
        "                        xy=pos[u], xycoords='data',\n",
        "                        xytext=pos[v], textcoords='data',\n",
        "                        arrowprops=dict(arrowstyle='-|>', color=kwargs.get('edge_color', 'k'),\n",
        "                                        shrinkA=10, shrinkB=10,\n",
        "                                        patchA=None, patchB=None,\n",
        "                                        connectionstyle=f\"arc3,rad={rad_neg}\"),\n",
        "                        )\n",
        "            seen.add((u, v))\n",
        "            seen.add((v, u))\n",
        "        elif (v, u) not in G.edges():\n",
        "            # single edge straight line\n",
        "            ax.annotate(\"\",\n",
        "                        xy=pos[v], xycoords='data',\n",
        "                        xytext=pos[u], textcoords='data',\n",
        "                        arrowprops=dict(arrowstyle='-|>', color=kwargs.get('edge_color', 'k'),\n",
        "                                        shrinkA=10, shrinkB=10,\n",
        "                                        patchA=None, patchB=None),\n",
        "                        )\n",
        "\n",
        "# Build graph (same as before)\n",
        "G_nx = nx.DiGraph()\n",
        "for edge in pc_result.G.get_graph_edges():\n",
        "    i, j = edge.node1, edge.node2\n",
        "    ep1, ep2 = edge.endpoint1, edge.endpoint2\n",
        "    if ep1.name == 'TAIL' and ep2.name == 'ARROW':\n",
        "        G_nx.add_edge(str(i), str(j))\n",
        "    elif ep1.name == 'ARROW' and ep2.name == 'TAIL':\n",
        "        G_nx.add_edge(str(j), str(i))\n",
        "    elif ep1.name == 'ARROW' and ep2.name == 'ARROW':\n",
        "        G_nx.add_edge(str(i), str(j))\n",
        "        G_nx.add_edge(str(j), str(i))\n",
        "    else:\n",
        "        G_nx.add_edge(str(i), str(j))\n",
        "        G_nx.add_edge(str(j), str(i))\n",
        "\n",
        "pos = nx.spring_layout(G_nx, k=2.5, iterations=150, seed=42)\n",
        "\n",
        "plt.figure(figsize=(14, 12))\n",
        "nx.draw_networkx_nodes(G_nx, pos,\n",
        "                       node_size=800,\n",
        "                       node_color='skyblue',\n",
        "                       edgecolors='black',\n",
        "                       linewidths=1.5)\n",
        "nx.draw_networkx_labels(G_nx, pos, font_size=10, font_color='black')\n",
        "\n",
        "# Draw edges with curvature for bidirectional edges\n",
        "draw_curved_edges(G_nx, pos, rad=0.3, edge_color='navy')\n",
        "\n",
        "plt.title(\"Causal Graph from PC over Latents\", fontsize=16)\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"causal_graph_pc.png\", dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lJk9-H_ArGZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of nodes:\", G_nx.number_of_nodes())\n",
        "print(\"Number of edges:\", G_nx.number_of_edges())"
      ],
      "metadata": {
        "id": "3wnR1btxiyIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For each node, get its parents from the graph\n",
        "parents_dict = {}\n",
        "for node in G_nx.nodes():\n",
        "    parents = list(G_nx.predecessors(node))\n",
        "    parents_dict[node] = parents"
      ],
      "metadata": {
        "id": "CC6sEVMPx2jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_causal_features(latents, parents_dict):\n",
        "    n_samples, n_latents = latents.shape\n",
        "    causal_feats = []\n",
        "\n",
        "    for node_str in sorted(parents_dict.keys(), key=lambda x: int(x[1:])):\n",
        "        node_idx = int(node_str[1:])\n",
        "        parent_indices = [int(p[1:]) for p in parents_dict[node_str]]\n",
        "\n",
        "        # Start with the node itself\n",
        "        combined = latents[:, node_idx:node_idx+1]\n",
        "\n",
        "        # Concatenate parent latents\n",
        "        if parent_indices:\n",
        "            parents_latents = latents[:, parent_indices]\n",
        "            combined = torch.cat([combined, parents_latents], dim=1)\n",
        "\n",
        "        # Example aggregation: sum over combined features (customize as needed)\n",
        "        feat = combined.sum(dim=1, keepdim=True)  # shape (batch_size, 1)\n",
        "        causal_feats.append(feat)\n",
        "\n",
        "    return torch.cat(causal_feats, dim=1)  # shape (batch_size, n_latents)\n"
      ],
      "metadata": {
        "id": "qf5HEWO2x76W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.classifier(x)"
      ],
      "metadata": {
        "id": "1J76iIsRzAhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run one batch to get the shape of causal features\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for axial, coronal, sagittal, labels in train_loader:\n",
        "        axial = axial.to(device).float()\n",
        "        coronal = coronal.to(device).float()\n",
        "        sagittal = sagittal.to(device).float()\n",
        "        mask = create_masks(labels.size(0), mode).to(device)\n",
        "\n",
        "        outputs, latents = model(axial, coronal, sagittal, mask, return_latent=True)\n",
        "        causal_features = create_causal_features(latents, parents_dict)\n",
        "\n",
        "        if isinstance(causal_features, np.ndarray):\n",
        "            causal_features = torch.tensor(causal_features, dtype=torch.float32, device=device)\n",
        "        else:\n",
        "            causal_features = causal_features.float().to(device)\n",
        "\n",
        "        input_dim = causal_features.shape[1]  # Number of latent features per sample\n",
        "        break  # only need first batch"
      ],
      "metadata": {
        "id": "-m7G3wzPS1RU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "num_classes = 11\n",
        "classifier = CausalClassifier(input_dim, num_classes).to(device)\n",
        "optimizer = torch.optim.Adam(list(model.parameters()) + list(classifier.parameters()), lr=1e-3)\n",
        "\n",
        "# Track metrics\n",
        "all_preds_final = []\n",
        "all_labels_final = []\n",
        "\n",
        "num_epochs = 20\n",
        "lambda_causal = 1e-2  # You can adjust this\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    classifier.train()\n",
        "    total_loss = 0\n",
        "    epoch_preds = []\n",
        "    epoch_labels = []\n",
        "\n",
        "    for axial, coronal, sagittal, labels in train_loader:\n",
        "        axial = axial.to(device).float()\n",
        "        coronal = coronal.to(device).float()\n",
        "        sagittal = sagittal.to(device).float()\n",
        "        labels = labels.to(device)\n",
        "        mask = create_masks(labels.size(0), mode).to(device)\n",
        "\n",
        "        outputs, latents = model(axial, coronal, sagittal, mask, return_latent=True)\n",
        "        causal_features = create_causal_features(latents, parents_dict)\n",
        "\n",
        "        if isinstance(causal_features, np.ndarray):\n",
        "            causal_features = torch.tensor(causal_features, dtype=torch.float32, device=device)\n",
        "        else:\n",
        "            causal_features = causal_features.float().to(device)\n",
        "\n",
        "        preds_causal = classifier(causal_features)\n",
        "        loss_cls = criterion(preds_causal, labels)\n",
        "        loss_causal = causal_reg(latents)\n",
        "        loss = loss_cls + lambda_causal * loss_causal\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * labels.size(0)\n",
        "\n",
        "        epoch_preds.extend(preds_causal.argmax(dim=1).detach().cpu().numpy())\n",
        "        epoch_labels.extend(labels.detach().cpu().numpy())\n",
        "\n",
        "    epoch_acc = accuracy_score(epoch_labels, epoch_preds)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {total_loss / len(train_loader.dataset):.4f} - Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "# --- Final Evaluation & Confusion Matrix ---\n",
        "model.eval()\n",
        "classifier.eval()\n",
        "with torch.no_grad():\n",
        "    for axial, coronal, sagittal, labels in val_loader:\n",
        "        axial = axial.to(device).float()\n",
        "        coronal = coronal.to(device).float()\n",
        "        sagittal = sagittal.to(device).float()\n",
        "        labels = labels.to(device)\n",
        "        mask = create_masks(labels.size(0), mode).to(device)\n",
        "\n",
        "        outputs, latents = model(axial, coronal, sagittal, mask, return_latent=True)\n",
        "        causal_features = create_causal_features(latents, parents_dict)\n",
        "\n",
        "        if isinstance(causal_features, np.ndarray):\n",
        "            causal_features = torch.tensor(causal_features, dtype=torch.float32, device=device)\n",
        "        else:\n",
        "            causal_features = causal_features.float().to(device)\n",
        "\n",
        "        preds = classifier(causal_features)\n",
        "        preds_labels = preds.argmax(dim=1)\n",
        "\n",
        "        all_preds_final.extend(preds_labels.cpu().numpy())\n",
        "        all_labels_final.extend(labels.cpu().numpy())\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(all_labels_final, all_preds_final)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix (Final Epoch)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"confusion_matrix_final.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Metrics\n",
        "acc = accuracy_score(all_labels_final, all_preds_final)\n",
        "prec = precision_score(all_labels_final, all_preds_final, average='macro', zero_division=0)\n",
        "rec = recall_score(all_labels_final, all_preds_final, average='macro', zero_division=0)\n",
        "f1 = f1_score(all_labels_final, all_preds_final, average='macro', zero_division=0)\n",
        "\n",
        "print(f\"Final Accuracy:  {acc:.4f}\")\n",
        "print(f\"Precision (macro): {prec:.4f}\")\n",
        "print(f\"Recall (macro):    {rec:.4f}\")\n",
        "print(f\"F1 Score (macro):  {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "x5yDRV1vzSgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Test Set Evaluation ---\n",
        "all_preds_test = []\n",
        "all_labels_test = []\n",
        "\n",
        "model.eval()\n",
        "classifier.eval()\n",
        "with torch.no_grad():\n",
        "    for axial, coronal, sagittal, labels in test_loader:\n",
        "        axial = axial.to(device).float()\n",
        "        coronal = coronal.to(device).float()\n",
        "        sagittal = sagittal.to(device).float()\n",
        "        labels = labels.to(device)\n",
        "        mask = create_masks(labels.size(0), mode).to(device)\n",
        "\n",
        "        outputs, latents = model(axial, coronal, sagittal, mask, return_latent=True)\n",
        "        causal_features = create_causal_features(latents, parents_dict)\n",
        "\n",
        "        if isinstance(causal_features, np.ndarray):\n",
        "            causal_features = torch.tensor(causal_features, dtype=torch.float32, device=device)\n",
        "        else:\n",
        "            causal_features = causal_features.float().to(device)\n",
        "\n",
        "        preds = classifier(causal_features)\n",
        "        preds_labels = preds.argmax(dim=1)\n",
        "\n",
        "        all_preds_test.extend(preds_labels.cpu().numpy())\n",
        "        all_labels_test.extend(labels.cpu().numpy())\n",
        "\n",
        "# Test Confusion Matrix\n",
        "cm_test = confusion_matrix(all_labels_test, all_preds_test)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm_test, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix with Causally Informed Representation in Random Mode(Test Set)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"confusion_matrix_test.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Test Metrics\n",
        "acc_test = accuracy_score(all_labels_test, all_preds_test)\n",
        "prec_test = precision_score(all_labels_test, all_preds_test, average='macro', zero_division=0)\n",
        "rec_test = recall_score(all_labels_test, all_preds_test, average='macro', zero_division=0)\n",
        "f1_test = f1_score(all_labels_test, all_preds_test, average='macro', zero_division=0)\n",
        "\n",
        "print(f\"Test Accuracy:  {acc_test:.4f}\")\n",
        "print(f\"Test Precision (macro): {prec_test:.4f}\")\n",
        "print(f\"Test Recall (macro):    {rec_test:.4f}\")\n",
        "print(f\"Test F1 Score (macro):  {f1_test:.4f}\")\n"
      ],
      "metadata": {
        "id": "ab5hS_XQp49T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Test Set Evaluation ---\n",
        "all_preds_test = []\n",
        "all_labels_test = []\n",
        "\n",
        "model.eval()\n",
        "classifier.eval()\n",
        "with torch.no_grad():\n",
        "    for axial, coronal, sagittal, labels in test_loader:\n",
        "        axial = axial.to(device).float()\n",
        "        coronal = coronal.to(device).float()\n",
        "        sagittal = sagittal.to(device).float()\n",
        "        labels = labels.to(device)\n",
        "        mask = create_masks(labels.size(0), mode='single').to(device)\n",
        "\n",
        "        outputs, latents = model(axial, coronal, sagittal, mask, return_latent=True)\n",
        "        causal_features = create_causal_features(latents, parents_dict)\n",
        "\n",
        "        if isinstance(causal_features, np.ndarray):\n",
        "            causal_features = torch.tensor(causal_features, dtype=torch.float32, device=device)\n",
        "        else:\n",
        "            causal_features = causal_features.float().to(device)\n",
        "\n",
        "        preds = classifier(causal_features)\n",
        "        preds_labels = preds.argmax(dim=1)\n",
        "\n",
        "        all_preds_test.extend(preds_labels.cpu().numpy())\n",
        "        all_labels_test.extend(labels.cpu().numpy())\n",
        "\n",
        "# Test Confusion Matrix\n",
        "cm_test = confusion_matrix(all_labels_test, all_preds_test)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm_test, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix with Causally Informed Representation in Single Mode (Test Set)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"confusion_matrix_test.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Test Metrics\n",
        "acc_test = accuracy_score(all_labels_test, all_preds_test)\n",
        "prec_test = precision_score(all_labels_test, all_preds_test, average='macro', zero_division=0)\n",
        "rec_test = recall_score(all_labels_test, all_preds_test, average='macro', zero_division=0)\n",
        "f1_test = f1_score(all_labels_test, all_preds_test, average='macro', zero_division=0)\n",
        "\n",
        "print(f\"Test Accuracy:  {acc_test:.4f}\")\n",
        "print(f\"Test Precision (macro): {prec_test:.4f}\")\n",
        "print(f\"Test Recall (macro):    {rec_test:.4f}\")\n",
        "print(f\"Test F1 Score (macro):  {f1_test:.4f}\")\n"
      ],
      "metadata": {
        "id": "r4qiSKT4wLuR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}